{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: \n",
    "\n",
    " This section was originally written to play around with the data from a single participant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "print(sys.executable) # This is where this notebook is running in\n",
    "print(sys.path) # This is where the downloaded modules are - sys.executable and sys.path need to be the same to be able to load the module.\n",
    "\n",
    "# Solution:\n",
    "# Run this in Terminal: (output of sys.executable) -m pip install (package name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages to convert formats\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import pingouin\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from numpy.core import einsumfunc\n",
    "import utility as ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File where the NMA fMRI data are\n",
    "path_data = \"/Volumes/DPhil_Jelka/fmri_data/hcp_task\"\n",
    "\n",
    "try: \n",
    "    os.chdir(path_data) \n",
    "except OSError as error: \n",
    "    print(error)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if you successfully changed directory\n",
    "os.getcwd() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, you want hcp_task to be your specific directory for now\n",
    "HCP_DIR = \"/Volumes/DPhil_Jelka/fmri_data/hcp_task\"\n",
    "if not os.path.isdir(HCP_DIR):\n",
    "    os.mkdir(HCP_DIR)\n",
    "\n",
    "# The data shared for NMA projects is a subset of the full HCP dataset (0-338)\n",
    "N_SUBJECTS = 339\n",
    "\n",
    "# The data have already been aggregated into ROIs from the Glasser parcellation\n",
    "N_PARCELS = 360\n",
    "\n",
    "# The acquisition parameters for all tasks were identical\n",
    "TR = 0.72  # Time resolution, in seconds\n",
    "\n",
    "# The parcels are matched across hemispheres with the same order\n",
    "HEMIS = [\"Right\", \"Left\"]\n",
    "\n",
    "# Each experiment was repeated twice in each subject\n",
    "N_RUNS = 2\n",
    "\n",
    "# There are 7 tasks. Each has a number of 'conditions'\n",
    "\n",
    "EXPERIMENTS = {\n",
    "    'MOTOR'      : {'runs': [5,6],   'cond':['lf','rf','lh','rh','t','cue']},\n",
    "    'WM'         : {'runs': [7,8],   'cond':['0bk_body','0bk_faces','0bk_places','0bk_tools','2bk_body','2bk_faces','2bk_places','2bk_tools']},\n",
    "    'EMOTION'    : {'runs': [9,10],  'cond':['fear','neut']},\n",
    "    'GAMBLING'   : {'runs': [11,12], 'cond':['loss','win']},\n",
    "    'LANGUAGE'   : {'runs': [13,14], 'cond':['math','story']},\n",
    "    'RELATIONAL' : {'runs': [15,16], 'cond':['match','relation']},\n",
    "    'SOCIAL'     : {'runs': [17,18], 'cond':['mental','rnd']}\n",
    "}\n",
    "\n",
    "# You may want to limit the subjects used during code development. This will use all subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load one .npy timeseries file\n",
    "\n",
    "def load_single_timeseries(subject, experiment, run, remove_mean=True):\n",
    "    \"\"\"\n",
    "    Load timeseries data for a single subject and single run.\n",
    "  \n",
    "    Args:\n",
    "    subject (int):      0-based subject ID to load\n",
    "    experiment (str):   Name of experiment \n",
    "    run (int):          0-based run index, across all tasks\n",
    "    remove_mean (bool): If True, subtract the parcel-wise mean (typically the mean BOLD signal is not of interest)\n",
    "\n",
    "    Returns\n",
    "    ts (n_parcel x n_timepoint array): Array of BOLD data values\n",
    "\n",
    "    \"\"\"\n",
    "    bold_run  = EXPERIMENTS[experiment]['runs'][run]\n",
    "    bold_path = f\"{HCP_DIR}/subjects/{subject}/timeseries\"\n",
    "    bold_file = f\"bold{bold_run}_Atlas_MSMAll_Glasser360Cortical.npy\"\n",
    "    ts = np.load(f\"{bold_path}/{bold_file}\")\n",
    "    if remove_mean:\n",
    "        ts -= ts.mean(axis=1, keepdims=True)\n",
    "    return ts\n",
    "\n",
    "\n",
    "def load_evs(subject, experiment, run):\n",
    "    \"\"\"Load EVs (explanatory variables) data for one task experiment.\n",
    "\n",
    "    Args:\n",
    "    subject (int): 0-based subject ID to load\n",
    "    experiment (str) : Name of experiment\n",
    "\n",
    "    Returns\n",
    "    evs (list of lists): A list of frames associated with each condition\n",
    "\n",
    "    \"\"\"\n",
    "    frames_list = []\n",
    "    task_key = 'tfMRI_'+ experiment + '_'+['RL','LR'][run]\n",
    "    for cond in EXPERIMENTS[experiment]['cond']:    \n",
    "        ev_file  = f\"{HCP_DIR}/subjects/{subject}/EVs/{task_key}/{cond}.txt\"\n",
    "        ev_array = np.loadtxt(ev_file, ndmin=2, unpack=True)\n",
    "        ev       = dict(zip([\"onset\", \"duration\", \"amplitude\"], ev_array))\n",
    "    \n",
    "    # Determine when trial starts, rounded down\n",
    "        start = np.floor(ev[\"onset\"] / TR).astype(int)\n",
    "    \n",
    "    # Use trial duration to determine how many frames to include for trial\n",
    "        duration = np.ceil(ev[\"duration\"] / TR).astype(int)\n",
    "    \n",
    "    # Take the range of frames that correspond to this specific trial\n",
    "        frames = [s + np.arange(0, d) for s, d in zip(start, duration)]\n",
    "        frames_list.append(frames)\n",
    "    \n",
    "    return frames_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_exp  = 'WM'\n",
    "my_subj = 2\n",
    "my_run  = 0\n",
    "\n",
    "data_example = load_single_timeseries(subject = my_subj, experiment = my_exp, run = my_run, remove_mean = True)\n",
    "\n",
    "print(data_example.shape) # 360 ROIs, 405 timepoints\n",
    "print(data_example) # np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info about regions\n",
    "regions = np.load(f\"{HCP_DIR}/regions.npy\").T\n",
    "region_info = dict(\n",
    "    name=regions[0].tolist(),\n",
    "    network=regions[1],\n",
    "    hemi=['Right']*int(N_PARCELS/2) + ['Left']*int(N_PARCELS/2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want ROIs in columns\n",
    "regions_T = np.transpose(regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many networks are there\n",
    "from collections import Counter\n",
    "\n",
    "# Function to get unique values\n",
    "def unique(list1):\n",
    "   \n",
    "    # Print directly by using * symbol\n",
    "    print(*Counter(list1))\n",
    "\n",
    "unique(regions[1]) #12 networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load explanatory variables - No clue what they current mean\n",
    "evs = load_evs(subject = my_subj, experiment = my_exp, run = my_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn regions into a pd.dataframe\n",
    "df_regions = pd.DataFrame(regions_T, columns = ['ROI', 'Network', 'Hemi'])\n",
    "print(df_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how many regions you have for each network\n",
    "df_regions['Network'].value_counts() # Not even close to an equal number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, we want ROIs in columns\n",
    "data_example_T = np.transpose(data_example)\n",
    "data_example_df = pd.DataFrame(data_example_T, columns = [df_regions['ROI'], df_regions['Network']])\n",
    "print(data_example_df.shape) #405 time points, 360 ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform original data_example to a pd.DataFrame\n",
    "data_example_df_original = pd.DataFrame(data_example)\n",
    "print(data_example_df_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge ROIs and networks\n",
    "Regions_Neural = df_regions.merge(data_example_df_original, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Regions_Neural.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_Network = np.arange(0, 360)\n",
    "print(index_Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_Network = Regions_Neural.columns\n",
    "print(columns_Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncorrected full correlations between ROIs\n",
    "ROI_CorrMatrix_Full = data_example_df.corr()\n",
    "print(ROI_CorrMatrix_Full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap - Didn't run, matrix is too big\n",
    "sn.heatmap(ROI_CorrMatrix_Full, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncorrected partial correlations between ROIs\n",
    "ROI_CorrMatrix_Partial = data_example_df.pcorr()\n",
    "print(ROI_CorrMatrix_Partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following procedure normalizes the response within ROIs over time\n",
    "scaler = preprocessing.StandardScaler().fit(data_example_df)\n",
    "data_example_df_zscore = scaler.transform(data_example_df)\n",
    "data_example_df_zscore = pd.DataFrame(data_example_df_zscore, columns = data_example_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check - Mean\n",
    "np.mean(data_example_df_zscore['R_4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check - SD\n",
    "np.std(data_example_df_zscore['R_4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "\n",
    "Data for all participants and tasks (0-back and 2-back) are downloaded, pre-processed and analysed from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add modules folder to Python's search path\n",
    "from os import times\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from os.path import dirname, realpath, abspath\n",
    "script_dir = Path(abspath(''))\n",
    "module_dir = str(script_dir)\n",
    "sys.path.insert(0, module_dir + '/modules')\n",
    "print(module_dir)\n",
    "print(script_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import task\n",
    "db_path = '{}/data/hcp_task'.format(module_dir)\n",
    "group = task.Group(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 0-back data - X are the features, y are the labels\n",
    "X_0back, y_0back = group.extract_cons(['0bk_faces', '0bk_tools', '0bk_places', '0bk_body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "print(X_0back.shape, y_0back.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 2-back data\n",
    "X_2back, y_2back = group.extract_cons(['2bk_faces', '2bk_tools', '2bk_places', '2bk_body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X_2back.shape, y_2back.shape)\n",
    "\n",
    "# 28080 - 360 (number of ROIs) x 78 (length of the time series)\n",
    "# 1356 - 339 (number of subjects) x 4 (number of conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "\n",
    "Ignore the section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate arrays with ROI names and time stamps\n",
    "ROIs = np.array(df_regions['ROI'])\n",
    "ROIs_Full = pd.DataFrame(np.repeat(ROIs, 78), columns = ['ROI'])\n",
    "\n",
    "Timepoints = np.array(range(1,79))\n",
    "Timepoints_Full = pd.DataFrame(np.tile(Timepoints, 360), columns = ['Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two and create a combined column\n",
    "ROIs_t = ROIs_Full.merge(Timepoints_Full, left_index=True, right_index=True)\n",
    "ROIs_t['Time'] = ROIs_t['Time'].astype(str)\n",
    "ROIs_t['ROI_t'] = ROIs_t['ROI'].str.cat(ROIs_t['Time'], sep ='_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "print(ROIs_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the features space to pd.DataFrame\n",
    "X_0back_df = pd.DataFrame(X_0back, columns = [ROIs_t['ROI_t']])\n",
    "print(X_0back_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the following column: Participant \n",
    "X_0back_df['Participant'] = np.repeat(np.array(range(0,339)), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the following column: Condition\n",
    "X_0back_df['Condition'] = pd.Series(np.tile(np.array(['faces', 'tools', 'places', 'body']), 339))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "print(X_0back_df.head())\n",
    "X_0back_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if you can run classifiers in this data structure\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# GLM\n",
    "Classification = LogisticRegressionCV(cv = 8, random_state = 0, multi_class = 'multinomial', max_iter = 10000).fit(X_0back_df.iloc[:, 0:28], X_0back_df.iloc[:, 28081])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "from sklearn import svm\n",
    "\n",
    "svm.SVC().fit(X_0back_df.iloc[:, 0:28], X_0back_df.iloc[:, 28081])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "\n",
    "Functional code for reshaping the dataframe can be found below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rois = regions[0]\n",
    "subjects = np.arange(339)\n",
    "conditions_0back = ['faces', 'tools', 'places', 'body']\n",
    "conditions_2back = ['faces', 'tools', 'places', 'body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the dataframe\n",
    "index_0back = []\n",
    "for cond in conditions_0back:\n",
    "    for subj in subjects:\n",
    "        for roi in rois:\n",
    "            index_0back.append((cond, subj, roi))\n",
    "len(index_0back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_2back = []\n",
    "for cond in conditions_2back:\n",
    "    for subj in subjects:\n",
    "        for roi in rois:\n",
    "            index_2back.append((cond, subj, roi))\n",
    "len(index_2back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the 0-back task dataframe\n",
    "X_new_0back = X_0back.reshape(488160, 78)\n",
    "\n",
    "index_0back = pd.MultiIndex.from_tuples(index_0back)\n",
    "X_df_0back = pd.DataFrame(X_new_0back, index = index_0back)\n",
    "X_df_0back.index.names = ['Condition', 'Subject_id', 'ROI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "print(X_df_0back.head())\n",
    "print(X_df_0back.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the 2-back task dataframe\n",
    "X_new_2back = X_2back.reshape(488160, 78)\n",
    "\n",
    "index_2back = pd.MultiIndex.from_tuples(index_2back)\n",
    "X_df_2back = pd.DataFrame(X_new_2back, index = index_2back)\n",
    "X_df_2back.index.names = ['Condition', 'Subject_id', 'ROI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "print(X_df_2back.head())\n",
    "print(X_df_2back.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to unpack indices (to convert them to columns) - 0-back\n",
    "X_df_0back.reset_index(level=0, inplace=True)\n",
    "X_df_0back.reset_index(level=0, inplace=True)\n",
    "X_df_0back.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to unpack indices (to convert them to columns) - 2-back\n",
    "X_df_2back.reset_index(level=0, inplace=True)\n",
    "X_df_2back.reset_index(level=0, inplace=True)\n",
    "X_df_2back.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the network column\n",
    "df_regions.reset_index(level=0, inplace=True)\n",
    "print(df_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "print(X_df_0back.head())\n",
    "print(X_df_0back.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the information about the brain network to the data frame\n",
    "X_0back_full = X_df_0back.merge(df_regions, left_on='ROI', right_on='ROI')\n",
    "X_2back_full = X_df_2back.merge(df_regions, left_on='ROI', right_on='ROI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "print(X_0back_full.head())\n",
    "print(X_2back_full.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by condition, network and participant and calculate the mean network activity at each of the 78 time steps\n",
    "X_0back_full_net = X_0back_full.groupby(['Subject_id', 'Condition', 'Network']).mean()\n",
    "X_2back_full_net = X_2back_full.groupby(['Subject_id', 'Condition', 'Network']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 339 (subj) x 4 (cond) x 12 (net) = 16272 rows\n",
    "X_0back_full_net.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "X_0back_full_net.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return indices to columns (0-back)\n",
    "X_0back_full_net.reset_index(level=0, inplace=True)\n",
    "X_0back_full_net.reset_index(level=0, inplace=True)\n",
    "X_0back_full_net.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return indices to columns (2-back)\n",
    "X_2back_full_net.reset_index(level=0, inplace=True)\n",
    "X_2back_full_net.reset_index(level=0, inplace=True)\n",
    "X_2back_full_net.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "X_0back_full_net.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "X_2back_full_net.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the column you don't need\n",
    "del X_0back_full_net[\"index\"]\n",
    "del X_2back_full_net[\"index\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "\n",
    "Ignore the code below - DO NOT rename the columns as that will throw an error when you try to subtract the 2-back and 0-back dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename time point columns\n",
    "cols_0back = X_0back_full_net.columns\n",
    "cols_2back = X_2back_full_net.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_0back = np.asarray(cols_0back, dtype=str)\n",
    "cols_2back = np.asarray(cols_2back, dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_0back = np.core.defchararray.add(cols_0back, '_0_back')\n",
    "cols_2back = np.core.defchararray.add(cols_2back, '_2_back')\n",
    "\n",
    "cols_0back[0] = 'Network'\n",
    "cols_0back[1] = 'Condition'\n",
    "cols_0back[2] = 'Subject_id'\n",
    "\n",
    "cols_2back[0] = 'Network'\n",
    "cols_2back[1] = 'Condition'\n",
    "cols_2back[2] = 'Subject_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "print(cols_0back)\n",
    "print(cols_2back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the column names\n",
    "X_0back_full_net.columns = cols_0back\n",
    "X_2back_full_net.columns = cols_2back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "X_0back_full_net.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "X_2back_full_net.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge 0-back and 2-back tasks\n",
    "X_0back_2back = X_0back_full_net.merge(X_2back_full_net, left_on=['Network', 'Condition', 'Subject_id'], right_on=['Network', 'Condition', 'Subject_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "X_0back_2back.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "\n",
    "Create a contrast dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrast 2-back - 0-back\n",
    "X_0back_full_net = X_0back_full_net.set_index(['Network', 'Condition', 'Subject_id'])\n",
    "X_2back_full_net = X_2back_full_net.set_index(['Network', 'Condition', 'Subject_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "X_0back_full_net.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "X_2back_full_net.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a contrast dataframe\n",
    "X_2back_0back_contr = X_2back_full_net - X_0back_full_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2back_0back_contr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return indices to columns (2-back)\n",
    "X_2back_0back_contr.reset_index(level=0, inplace=True)\n",
    "X_2back_0back_contr.reset_index(level=0, inplace=True)\n",
    "X_2back_0back_contr.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NEXT STEPS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "# NORMALIZING DATA - It is very important to figure out what is meant by this and how to execute it.\n",
    "# NORMALIZING DATA - This might probably need to be done before creating contrasts or anything.\n",
    "# WITHIN-NETWORK CLASSIFICATION - Doing the classification within each network + Regularization?\n",
    "# CROSS-VALIDATION - Are we 'manually' splitting the data into train/test or is the function doing that for us?\n",
    "# REGULARIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn imports\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLM - THIS IS NOT SPECIFIED CORRECTLY, IT NEEDS TO BE UPDATED.\n",
    "Classification_Contrast = LogisticRegressionCV(cv = 8, random_state = 0, multi_class = 'multinomial', max_iter = 10000).fit(X_2back_0back_contr.iloc[:, 3:81], X_2back_0back_contr.iloc[:, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "\n",
    "To be continued."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
